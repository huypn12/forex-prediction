\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\title{Foreign Exchange Rates forcasting with LSTM}
\author{Huy Phung, Tashi Choden, Sahil Pasricha
  \\University of Konstanz}
\date{February 2019}


\begin{document}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Abstract}
Foreign Exchange (abbreviation \textit{Forex} or simply \textit{FX}) Market is
the decentralized market for currency investment. Forex market is the second
most important market, after stock market. Supply and demand in the market
determine Forex rate, in which a pair of currency can be exchanged. Forex Rates
has been studied in econometrics as a financial timeseries. The purpose of
studying Forex rates is to explain the market behaviour or forecast future
prices.\\ 
In our project, we use statistical models established in econometrics and deep
learning model to predict the future rates of one step ahead. Our goal is to
compare the effectiveness of LSTM and statistical models (ARIMA and VAR) as
timeseries models, in both accuracy and performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Description}
\subsection{Forex rates}
Foreign Exchange rates (short Forex rates) are decided solely by support and
demand of Forex market. Each rate represents the price to buy or sell a pair of
currency (e.g. EURUSD) at the moment. The price to buy is called Bid price; the
price to sell is called Ask price. The difference between Bid price and Ask
price is called Spread. In this project we consider only the Bid price. However,
if both Bid and Ask (and therefore Spread) were available, our analysis would be
more precise.\\
Forex brokers update rates according to the market
within milliseconds by standardized FIX (Financial Information eXchange)
protocol. The time interval between each FIX message to update rates is not
uniform; it may varies from a millisecond to few seconds. Therefore, the
timeseries is of continuous time step. In order to simplify our
analysis, we convert it to a data form that has discrete, uniform time
step, while still keep important information.\\
One possible way to do so is to format the rates into OHLC format. This approach
is widely used in financial technical analysis. We partition the timeseries into
intervals of uniform time $t$, 


\subsection{Dataset}
Cite published a dataset consists of OHLC of BID price (lack of ASK price)

%% TODO add additional features: avg price, volume, pca

\subsection{Forecasting}
In this project, we concern about the prediction of future Open, High, Low,
Close prices. Other features, either originally exists (volume) or later added
(mean, median, momentum), are only considered as supporting features. These
features are only used for prediction of OHLC features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model selecion}
\subsubsection{Akaike Information Criterion (AIC)}
For statistical model selection, we use Akaike Information Criterion (AIC). 
As we can see, AIC penalize a model by its number of parameters. The
number of parameters of a neural network based model is at least total number of
elements contained in all weights in its topology. Therefore, we should not use
AIC as a measure of performance between statistical models and deep learning
models.
$$
AIC = 2k -2\ln(\hat{L})
$$

\subsubsection{Root Mean Squared Error (RMSE)}
Root Mean Squared Error is widely used to measure the difference between values
predicted by a model and the actually observed values. Given $y$ represents the
actually observed values and $\hat{y}$ represents the values predicted, $RMSE$
is given by:
$$
RMSE = \left( \frac{1}{n}\sum _{i=1}^{n}(y_i -\hat{y}_i)^2 \right)^\frac{1}{2}
$$
Root Mean Squared Error shows absolute difference between $y$ and $\hat{y}$.
However, since it does not take the range of possible values into account, it
would be difficult to intepret the $RMSE$ result without knowing the possible
range of predicted and actual values. 

\subsubsection{Mean Absolute Percentage Error (MAPE)}
In order to measure the difference between predicted values and actual values
with regarding to the scale, we use Mean Absolute Percentage Error (MAPE)
$$
MAPE = \frac{100\%}{n}\sum  _{i=1}^{n}\left| \frac{y_i -\hat{y}_i}{y_i} \right|
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Statistical Models}
\subsection{Autocorrelation}

\subsection{ARIMA}
\subsubsection{AR(p)}
ARIMA consists of 2 sub models: AR(p) and MA(q)

\subsubsection{MA(q)}

\subsubsection{ARIMA(p,d,q)}

\subsubsection{Parameters selection}
It is important for ARIMA model that we select the proper parameter $(p, d, q)$
so that it covers all the past values which has effect on the current value. In
order to do so, we first look at survey the ACF and PACF plot. For $AR(p)$ model
, we find the furthest lag with significant autocorrelation in ACF plot. For
$MA(q)$, we find at which lag the autocorrelation start to decay.

%% TODO add example figure


\subsubsection{Result}

\subsection{VAR(p)}
\subsubsection{Model description}
VAR is applied to multivariate data. It is similar to AR(p) model. However here
we consider 
\subsubsection{Parameters selection}
Parameters selection for VAR consists only testing a range of lag with AIC. 
\subsubsection{Result}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Deep Learning Model}
\subsection{Recurrent Neural Network}
Recurrent Neural Network (RNN) is introduced by \cite{rumelhart1988learning} to
process sequential input. In RNN, each state connects to the followed state to form a directed
graph. The structure of RNN makes it capable of handling sequential data with
temporal dynamic behaviour.
%% TODO graphics

One problem that may occurs within RNN is \textit{vanishing gradient}https://www.youtube.com/watch?v=AvQP1WMf1cs
\cite{hochreiter2001gradient}
%% TODO graphics

\subsection{Long-Short Term Memory}
Hochreiter and Schmidhuber (1997) \cite{gers1999learning} presented a network
architecture to solve the vanishing gradient problem from RNN
Cummins presented 

\subsection{Proposed Network Topology}
Kim and Elsaftawy \cite{kimy07lstm} proposed a LSTM structure 
%% TODO graphics
Our survey on the data implies that significant autocorrelation may appears even
further than 20 timesteps into the past. In our proposed network topology, we
add more hidden units into each LSTM layers, so that the network can learn from
data points of higher lags.

\subsection{Univariate and Multivariate}
We consider using LSTM network in two usecases. In the first usecases, we train
the network to predict future values only by giving to history lags of one
timeseries (univariate). For example, we train the network to predict 3 Open
prices in the future given only the Open prices in the past. This usecases is
designed to compare the performance with ARIMA(p, d, q) model.\\
In the second usecase, 

\subsection{Training and Validation}
As we can see from training progress, after the first epoch, the loss within the
network does not decrease any more, meanwhile the 
\subsection{Results}

\subsubsection{Single Variate}

\subsubsection{Multivariate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Conclusion}
%% TODO Compare the result between lstm and arima/varma
As the results have shown, ...

However, computational effort spent for training LSTM deep learning model is
much higher than ARIMA and VAR. Furthermore the, adding components to neural network
topology results in higher computational cost. 

\pagebreak
\bibliographystyle{plain}
\bibliography{report}

\end{document}